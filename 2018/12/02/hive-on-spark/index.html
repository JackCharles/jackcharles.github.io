<!DOCTYPE html>
<html lang="en">
  <head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="作为大数据开发人员，一直将精力放在业务上，很久没有关注大数据工具hive了。最近在写一个UDF函数时发现在hive(mr)引擎能通过，在spark引擎下却报错，于是就借此机会粗略研究一下hive on mr、hive on spark和spark sql等内容。本篇为整个系列开篇，介绍了MAC下Hive on spark的环境搭建，及遇到的坑。"><meta name="keywords" content="hive, spark, hadoop, 大数据, 江影不沉浮"><link rel="alternate" href="/atom.xml" title="江影不沉浮"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.0">
<link rel="canonical" href="https://blog.zjee.me/2018/12/02/hive-on-spark/">

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css"><link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css"><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0">

<script id="baidu_analytics">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?440ec0c403d1159e127d0a966dc3e964";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "hiuyEvHNGJv0HBzIgke1FOa2-MdYXbMMI",
      appKey: "W2WW9ah2D4zfyhW3aYtKw3xD"
    });
  </script><script>
  window.config = {"leancloud":{"app_id":"hiuyEvHNGJv0HBzIgke1FOa2-MdYXbMMI","app_key":"W2WW9ah2D4zfyhW3aYtKw3xD"},"toc":true,"fancybox":true,"pjax":true,"latex":true};
</script>

    <title>Hive on Spark 实践 - 江影不沉浮</title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">江影不沉浮</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">江影不沉浮</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">Hive on Spark 实践
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-02
        </span><span class="post-category">
            <a href="/categories/大数据/">大数据</a>
            </span>
        <span class="post-visits" data-url="/2018/12/02/hive-on-spark/" data-title="Hive on Spark 实践">
          Visits 0
        </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#一、准备"><span class="toc-text">一、准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二、配置SPARK"><span class="toc-text">二、配置SPARK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三、配置Hive"><span class="toc-text">三、配置Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#四、配置yarn"><span class="toc-text">四、配置yarn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#五、其他配置"><span class="toc-text">五、其他配置</span></a></li></ol>
    </div>
  </div><div class="post-content"><h3 id="一、准备"><a href="#一、准备" class="headerlink" title="一、准备"></a>一、准备</h3><ol>
<li>jdk、hadoop、Hive、mysql和spark。为避免兼容问题，本人全部采用cdh版本，对应cdh5.15.0，不过貌似spark版本有点低，不过无所谓，毕竟只是实验。</li>
<li>由于本人之前搭建过kylin环境，因此hadoop和hive环境已经配置妥当，都是yarn管理下的伪分布式集群。</li>
<li>安装scala，mac下通过brew安装：<code>brew install scala</code>，网速慢可以使用代理或<a href="https://www.cnblogs.com/jay54520/p/6347729.html" target="_blank" rel="noopener">国内源</a>。</li>
<li><a href="https://www.cnblogs.com/binarylei/p/8903601.html" target="_blank" rel="noopener">hadoop</a>、<a href="https://www.cnblogs.com/wujiadong2014/p/6058552.html" target="_blank" rel="noopener">hive</a>、<a href="https://www.jianshu.com/p/e41b18a7e202" target="_blank" rel="noopener">spark</a>的基本概念，原理，运行方式可做一个初步了解，方便以后问题排查。</li>
</ol>
<h3 id="二、配置SPARK"><a href="#二、配置SPARK" class="headerlink" title="二、配置SPARK"></a>二、配置SPARK</h3><ol>
<li><p>在/etc/profile或~/.bash_profile（假设你没有使用zsh之类的shell）中加入SPARK_HOME环境变量，并将bin路径添加到PATH中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/Users/xxx/spark-1.6.0-cdh5.15.0</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>在spark下的config目录中找到spark-env.sh和spark-defaults.conf(如果没有就自己创建)，然后添加如下配置:</p>
<p>spark-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> !/usr/bin/env bash</span><br><span class="line">export SCALA_HOME=/usr/local/Cellar/scala/2.12.7 #注意scala版本</span><br><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home</span><br><span class="line">export HADOOP_HOME=/Users/xxx/hadoop-2.6.0-cdh5.15.0</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #hadoop配置文件目录</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop #yarn配置文件目录(和hadoop配置在一起)</span><br><span class="line">export SPARK_MASTER_IP=localhost #master节点IP，本地搭建使用localhost</span><br><span class="line">export SPARK_MASTER_HOST=localhost #master机器名称，本地搭建可使用localhost代替</span><br><span class="line">export SPARK_EXECUTOR_MEMORY=512m #每个executor可分配的内存，可根据机器实际情况设置为512m到1g</span><br><span class="line">export SPARK_DRIVER_MEMORY=512m #driver可分配的内存，可根据机器实际情况设置为512m到1g</span><br><span class="line">export SPARK_DIST_CLASSPATH=$($&#123;HADOOP_HOME&#125;/bin/hadoop classpath) #saprk运行依赖hadoop的库</span><br></pre></td></tr></table></figure>

<p>spark-defaults.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     yarn</span><br><span class="line">spark.home                       /Users/xxx/spark-1.6.0-cdh5.15.0</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:8001/spark_history</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.executor.memory            512m</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这里将spark托管到yarn上，所以spark.master配置为yarn，日志目录必须是hdfs路径下的<strong><em>已经存在</em></strong>的目录，如果不存在请手动创建<code>hdfs dfs -mkdir /spark_history</code>，这个目录也是后面的spark history服务依赖的目录。</p>
</li>
</ol>
<h3 id="三、配置Hive"><a href="#三、配置Hive" class="headerlink" title="三、配置Hive"></a>三、配置Hive</h3><ol>
<li><p>将spark-defaults.conf中的配置同步到hive配置中，在hive下的conf目录下找到hive-site.xml，增加如下内容：<br> hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 注意：hive.execution.engine在这里设置表示hive默认使用spark引擎，</span></span><br><span class="line"><span class="comment">也可以不在这里设置，进入hive cli或客户端后使用set hive.execution.engine=spark</span></span><br><span class="line"><span class="comment">启用spark引擎，但这种方式只针对当前会话有效。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.home&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/Users/xxx/spark-1.6.0-cdh5.15.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.master&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.executor.memory&lt;/name&gt;</span><br><span class="line">&lt;value&gt;512m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.driver.memory&lt;/name&gt;</span><br><span class="line">&lt;value&gt;512m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.serializer&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.enentLog.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.enentLog.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://localhost:8001/spark_history&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.executor.extraJavaOptions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这里的配置务必和spark-defaults.conf中的一致，尤其是<code>spark.executor.memory</code>和<code>spark.driver.memory</code>，自己配置时填错了这两项，结果一堆莫名其妙的异常。</p>
</li>
</ol>
<h3 id="四、配置yarn"><a href="#四、配置yarn" class="headerlink" title="四、配置yarn"></a>四、配置yarn</h3><ol>
<li><p>配置yarn，在hadoop的etc/hadoop目录下找到yarn-site.xml，增加（修改）如下内容：<br>yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn资源调度器：公平调度 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM的web监控地址，主要是端口设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0:8042<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM可分派的内存量，单机的话看机器剩余内存量配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM可分配的虚拟CPU数，单机的话配置为CPU核数-2或-1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="五、其他配置"><a href="#五、其他配置" class="headerlink" title="五、其他配置"></a>五、其他配置</h3><ol>
<li><p>你以为这就完了？没有，CDH版本的Hive与Spark并没有联系，所以需要我们手动将spark lib中的<code>spark-assembly-1.6.0-cdh5.15.0-hadoop2.6.0-cdh5.15.0.jar</code>拷贝到hive lib下，hive才能与spark建立联系。</p>
</li>
<li><p>理论上一切OK了，但你可能会发现在hive中使用spark引擎时返回3号错误，总之就是spark运行不正常，这时可以使用spark自带的example进行测试诊断（cd到spark bin目下）：<code>./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-cdh5.15.0-hadoop2.6.0-cdh5.15.0.jar 10</code>，具体example jar路径及文件名自行查看。运行起来后有一个异常解决一个异常，直到跑出结果为止，没本人测试中发现以下问题：</p>
<blockquote>
<p><strong>没有jackson模块</strong>：WTF，官网下载的spark居然包都不带全，随后查了一下cdh版本确实没带jackson的相关包，然后去maven官网下载(三个都要下载)，放到<code>{HADOOP_HOME}/share/hadoop/common/lib</code>下，至于为什么放在这儿而不是spark lib，这是因为我们在前面配置了<code>SPARK_DIST_CLASSPATH</code>（看不明白就在shell中echo一下），它会加入到spark类路径下，这个路径就包含上面的<code>{HADOOP_HOME}/share/hadoop/common/lib</code>，而且它下面还有jackson的其他包，顺理成章就放这儿了。随后再试！</p>
</blockquote>
<blockquote>
<p><strong>jackson版本问题</strong>：WTF，这回找到jackson了，但jackson内部抛异常了，上网查发现是版本问题，于是按热心网友推荐采用2.4.4的版本，这回不抛异常了！</p>
</blockquote>
<blockquote>
<p><strong>无限ACCEPT</strong>：虽然异常是没了，可这个程序一直是ACCEPT（等一个小时了），然后又上网搜索找到部分答案，测试无果，再回头看前面的配置，发现sparkk-env.sh中<code>SPARK_MASTER_IP</code>和<code>SPARK_MASTER_HOST</code>跟那些傻屌网友一样配成了master，可是我的机器不叫master，所以yarn找不到执行机器，也就无法分配内存，遂更改为localhost(本篇文章开头配置已更改)，然后 everything is OK！<br></p>
</blockquote>
<blockquote>
<p>如果你的机器内存较小，导致分配资源缓慢，可适当调整一下<code>${HADOOP_HOME}/etc/hadoop/capacity-scheduler.xml</code>中的<code>yarn.scheduler.capacity.maximum-am-resource-percent</code>，可以由0.1调整为0.5，它表示yarn所管理的资源中，最多可以有多少资源可以用来运行application master，即控制当前激活状态的应用，默认是10%，在我们自己机器上来说太小了，因此可适当调大比例。</p>
</blockquote>
</li>
</ol>
<hr>
<ol start="3">
<li>终极测试：<img src="/2018/12/02/hive-on-spark/001.png" title="Hive on Spark 测试结果"></li>
</ol>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>Author: </span>
      <a href="https://blog.zjee.me">jackcharles</a>
    </p>
    <p class="copyright-item">
      <span>Link: </span>
      <a href="https://blog.zjee.me/2018/12/02/hive-on-spark/">https://blog.zjee.me/2018/12/02/hive-on-spark/</a>
    </p>
    <p class="copyright-item">
      <span>License: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/hive/">hive</a>
            <a href="/tags/spark/">spark</a>
            <a href="/tags/hadoop/">hadoop</a>
            <a href="/tags/大数据/">大数据</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2019/08/24/java-lock-util/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Java中的同步、互斥机制</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    <a class="next" href="/2018/11/29/thrift-grama/">
        <span class="next-text nav-default">RPC框架之Thrift</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments">
	<p style="color:#0047AB">
	这里是评论区，使用
	<a href="http://livere.com/" target="_blank">来必力(LiveRe)</a>
	插件实现，邮箱、昵称、密码可随意填写，但要修改、删除评论就必须登录LiveRe，您也可以使用第三方账号登录LiveRe。
	</p><div id="lv-container" data-id="city" data-uid="MTAyMC80MTIyNy8xNzc3NQ==">
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
      </div>  
    </div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:zjee@live.cn" class="iconfont icon-email" title="email"></a>
        <a href="/pass" class="iconfont icon-facebook" title="facebook"></a>
        <a href="/pass" class="iconfont icon-weibo" title="weibo"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">&copy;2020<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">jackcharles</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript">
	(function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
  </script><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
