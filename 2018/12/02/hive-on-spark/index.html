<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="作为大数据开发人员，一直将精力放在业务上，很久没有关注大数据工具hive了。最近在写一个UDF函数时发现在hive(mr)引擎能通过，在spark引擎下却报错，于是就借此机会粗略研究一下hive on mr、hive on spark和spark sql等内容。本篇为整个系列开篇，介绍了MAC下Hive on spark的环境搭建，及遇到的坑。"><meta name="keywords" content="大数据,hive,spark,hadoop"><meta name="author" content="Jack.Charles"><meta name="copyright" content="Jack.Charles"><title>Hive on Spark 实践 | 江影不沉浮</title><link rel="shortcut icon" href="/about/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3f20839b943a104368c94974ce5350fa";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'G-18T0HPMMWC', 'auto');
ga('send', 'pageview');</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.2.0'
} </script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="江影不沉浮" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%87%86%E5%A4%87"><span class="toc-text">一、准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%85%8D%E7%BD%AESPARK"><span class="toc-text">二、配置SPARK</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%85%8D%E7%BD%AEHive"><span class="toc-text">三、配置Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%85%8D%E7%BD%AEyarn"><span class="toc-text">四、配置yarn</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE"><span class="toc-text">五、其他配置</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/about/avatar.jpg"></div><div class="author-info__name text-center">Jack.Charles</div><div class="author-info__description text-center">记录职业生涯的点点滴滴...</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/JackCharles">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">18</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://api.dujin.org/bing/1920.php)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">江影不沉浮</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Hive on Spark 实践</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1.6k</span><span class="post-meta__separator">|</span><span>阅读时长: 6 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一、准备"><a href="#一、准备" class="headerlink" title="一、准备"></a>一、准备</h2><ol>
<li>jdk、hadoop、Hive、mysql和spark。为避免兼容问题，本人全部采用cdh版本，对应cdh5.15.0，不过貌似spark版本有点低，不过无所谓，毕竟只是实验。</li>
<li>由于本人之前搭建过kylin环境，因此hadoop和hive环境已经配置妥当，都是yarn管理下的伪分布式集群。</li>
<li>安装scala，mac下通过brew安装：<code>brew install scala</code>，网速慢可以使用代理或<a target="_blank" rel="noopener" href="https://www.cnblogs.com/jay54520/p/6347729.html">国内源</a>。</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/binarylei/p/8903601.html">hadoop</a>、<a target="_blank" rel="noopener" href="https://www.cnblogs.com/wujiadong2014/p/6058552.html">hive</a>、<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e41b18a7e202">spark</a>的基本概念，原理，运行方式可做一个初步了解，方便以后问题排查。</li>
</ol>
<h2 id="二、配置SPARK"><a href="#二、配置SPARK" class="headerlink" title="二、配置SPARK"></a>二、配置SPARK</h2><ol>
<li><p>在/etc/profile或~/.bash_profile（假设你没有使用zsh之类的shell）中加入SPARK_HOME环境变量，并将bin路径添加到PATH中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/Users/xxx/spark-1.6.0-cdh5.15.0</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>在spark下的config目录中找到spark-env.sh和spark-defaults.conf(如果没有就自己创建)，然后添加如下配置:</p>
<p>spark-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> !/usr/bin/env bash</span></span><br><span class="line">export SCALA_HOME=/usr/local/Cellar/scala/2.12.7 #注意scala版本</span><br><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home</span><br><span class="line">export HADOOP_HOME=/Users/xxx/hadoop-2.6.0-cdh5.15.0</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #hadoop配置文件目录</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop #yarn配置文件目录(和hadoop配置在一起)</span><br><span class="line">export SPARK_MASTER_IP=localhost #master节点IP，本地搭建使用localhost</span><br><span class="line">export SPARK_MASTER_HOST=localhost #master机器名称，本地搭建可使用localhost代替</span><br><span class="line">export SPARK_EXECUTOR_MEMORY=512m #每个executor可分配的内存，可根据机器实际情况设置为512m到1g</span><br><span class="line">export SPARK_DRIVER_MEMORY=512m #driver可分配的内存，可根据机器实际情况设置为512m到1g</span><br><span class="line">export SPARK_DIST_CLASSPATH=$($&#123;HADOOP_HOME&#125;/bin/hadoop classpath) #saprk运行依赖hadoop的库</span><br></pre></td></tr></table></figure>

<p>spark-defaults.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     yarn</span><br><span class="line">spark.home                       &#x2F;Users&#x2F;xxx&#x2F;spark-1.6.0-cdh5.15.0</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;localhost:8001&#x2F;spark_history</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.executor.memory            512m</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这里将spark托管到yarn上，所以spark.master配置为yarn，日志目录必须是hdfs路径下的 <strong><em>已经存在</em></strong>的目录，如果不存在请手动创建<code>hdfs dfs -mkdir /spark_history</code>，这个目录也是后面的spark history服务依赖的目录。</p>
</li>
</ol>
<h2 id="三、配置Hive"><a href="#三、配置Hive" class="headerlink" title="三、配置Hive"></a>三、配置Hive</h2><ol>
<li><p>将spark-defaults.conf中的配置同步到hive配置中，在hive下的conf目录下找到hive-site.xml，增加如下内容：<br> hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 注意：hive.execution.engine在这里设置表示hive默认使用spark引擎，</span></span><br><span class="line"><span class="comment">也可以不在这里设置，进入hive cli或客户端后使用set hive.execution.engine=spark</span></span><br><span class="line"><span class="comment">启用spark引擎，但这种方式只针对当前会话有效。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.home&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;Users&#x2F;xxx&#x2F;spark-1.6.0-cdh5.15.0&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.master&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.executor.memory&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;512m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.driver.memory&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;512m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.serializer&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.enentLog.enabled&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.enentLog.dir&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:8001&#x2F;spark_history&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.executor.extraJavaOptions&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;-XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：这里的配置务必和spark-defaults.conf中的一致，尤其是<code>spark.executor.memory</code>和<code>spark.driver.memory</code>，自己配置时填错了这两项，结果一堆莫名其妙的异常。</p>
</li>
</ol>
<h2 id="四、配置yarn"><a href="#四、配置yarn" class="headerlink" title="四、配置yarn"></a>四、配置yarn</h2><ol>
<li><p>配置yarn，在hadoop的etc/hadoop目录下找到yarn-site.xml，增加（修改）如下内容：<br>yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn资源调度器：公平调度 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM的web监控地址，主要是端口设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0:8042<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM可分派的内存量，单机的话看机器剩余内存量配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NM可分配的虚拟CPU数，单机的话配置为CPU核数-2或-1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="五、其他配置"><a href="#五、其他配置" class="headerlink" title="五、其他配置"></a>五、其他配置</h2><ol>
<li><p>CDH版本的Hive与Spark没有联系，需要我们手动将spark lib中的<code>spark-assembly-1.6.0-cdh5.15.0-hadoop2.6.0-cdh5.15.0.jar</code>拷贝到hive lib下，hive才能与spark建立联系。</p>
</li>
<li><p>使用spark自带的example进行测试诊断（cd到spark bin目下）：<code>./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-cdh5.15.0-hadoop2.6.0-cdh5.15.0.jar 10</code>，具体example jar路径及文件名自行查看。运行起来后有一个异常解决一个异常，直到跑出结果为止，测试中发现以下问题：</p>
<ul>
<li><p> <strong>没有jackson模块</strong>：CDH版本没带jackson的相关包，去maven官网下载(三个都要下载)，放到<code>&#123;HADOOP_HOME&#125;/share/hadoop/common/lib</code>下，至于为什么放在这儿而不是spark lib，这是因为我们在前面配置了<code>SPARK_DIST_CLASSPATH</code>（看不明白就在shell中echo一下），它会加入到spark类路径下，这个路径就包含上面的<code>&#123;HADOOP_HOME&#125;/share/hadoop/common/lib</code>，而且它下面还有jackson的其他包，顺理成章就放这儿了。</p>
</li>
<li><p><strong>jackson版本问题</strong>：Jackson内部抛异常，上网查发现是版本问题，于是按热心网友推荐采用2.4.4的版本，问题解决！</p>
</li>
<li><p><strong>无限ACCEPT</strong>：spark-env.sh中<code>SPARK_MASTER_IP</code>和<code>SPARK_MASTER_HOST</code>需要配置为自己机器的hostname或者localhost。</p>
</li>
<li><p>如果机器内存较小，导致分配资源缓慢，可适当调整一下<code>$&#123;HADOOP_HOME&#125;/etc/hadoop/capacity-scheduler.xml</code>中的<code>yarn.scheduler.capacity.maximum-am-resource-percent</code>，可以由0.1调整为0.5，它表示yarn所管理的资源中，最多可以有多少资源可以用来运行application master，即控制当前激活状态的应用，默认是10%，可适当调大比例。</p>
</li>
</ul>
</li>
<li><p>终极测试：<img src="/2018/12/02/hive-on-spark/001.png" class="" title="Hive on Spark 测试结果"></p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Jack.Charles</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.zjee.me/2018/12/02/hive-on-spark/">https://blog.zjee.me/2018/12/02/hive-on-spark/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.zjee.me">江影不沉浮</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/hive/">hive</a><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/hadoop/">hadoop</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/08/24/java-lock-util/"><i class="fa fa-chevron-left">  </i><span>Java中的同步、互斥机制</span></a></div><div class="next-post pull-right"><a href="/2018/11/29/thrift-grama/"><span>RPC框架之Thrift</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = '' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'hiuyEvHNGJv0HBzIgke1FOa2-MdYXbMMI',
  appKey:'W2WW9ah2D4zfyhW3aYtKw3xD',
  placeholder:'say something...',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://api.dujin.org/bing/1920.php)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By Jack.Charles</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>